{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42eb430f",
   "metadata": {},
   "source": [
    "# MCP demo (VS Code) – Setup progetto\n",
    "\n",
    "Questo notebook crea una demo **MCP client/server** *locale* e completamente **open/gratis**:\n",
    "\n",
    "- **MCP server arXiv** (tool: `arxiv_search`)\n",
    "- **MCP server OpenAlex** (tool: `openalex_search_works`)\n",
    "- **Client di test** che chiama entrambi e mostra i risultati\n",
    "\n",
    "> Nota: la modalità `stdio` richiede che i server girino come processi locali (perfetto per prototipare in VS Code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5905e3",
   "metadata": {},
   "source": [
    "## 0.1 – Crea cartelle progetto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb205999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/monica.costantini/Documents/workspace/mcp-demo')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "SERVERS = ROOT / \"servers\"\n",
    "CLI = ROOT / \"cli\"\n",
    "VSCODE = ROOT / \".vscode\"\n",
    "\n",
    "for p in [SERVERS, CLI, VSCODE]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390846e",
   "metadata": {},
   "source": [
    "## 0.2 – Scrivi i file dei due MCP server (arXiv + OpenAlex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e982dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creati: /Users/monica.costantini/Documents/workspace/mcp-demo/servers/arxiv_server.py e /Users/monica.costantini/Documents/workspace/mcp-demo/servers/openalex_server.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "arxiv_server = r'''from __future__ import annotations\n",
    "import arxiv\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"arxiv\")\n",
    "\n",
    "@mcp.tool()\n",
    "def arxiv_search(topic: str, max_results: int = 5):\n",
    "    \"\"\"Search arXiv by topic and return basic metadata.\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    for r in search.results():\n",
    "        out.append({\n",
    "            \"title\": r.title,\n",
    "            \"authors\": [a.name for a in r.authors],\n",
    "            \"published\": r.published.isoformat() if r.published else None,\n",
    "            \"pdf_url\": r.pdf_url,\n",
    "            \"entry_id\": r.entry_id,\n",
    "            \"summary\": r.summary,\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Non usare print() su stdout in stdio: rompe JSON-RPC.\n",
    "    mcp.run(transport=\"stdio\")\n",
    "'''\n",
    "\n",
    "openalex_server = r'''from __future__ import annotations\n",
    "import httpx\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"openalex\")\n",
    "BASE = \"https://api.openalex.org\"\n",
    "\n",
    "@mcp.tool()\n",
    "def openalex_search_works(query: str, per_page: int = 5, mode: str = \"ai\"):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - \"ai\": filtra per risultati AI/RAG/LLM (più precisione)\n",
    "      - \"general\": nessun filtro AI (più recall)\n",
    "    \"\"\"\n",
    "    url = f\"{BASE}/works\"\n",
    "\n",
    "    fetch_n = max(per_page * 10, 50)\n",
    "    params = {\"search\": query, \"per_page\": fetch_n}\n",
    "\n",
    "    r = httpx.get(url, params=params, timeout=30.0)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    keywords_ai = (\n",
    "        \"retrieval\",\n",
    "        \"generation\",\n",
    "        \"rag\",\n",
    "        \"language model\",\n",
    "        \"llm\",\n",
    "        \"agent\",\n",
    "        \"benchmark\",\n",
    "        \"transformer\",\n",
    "    )\n",
    "\n",
    "    negative_rag_bio = (\n",
    "        \"mice\", \"mouse\", \"murine\",\n",
    "        \"gene\", \"genes\", \"genetic\", \"immun\", \"chemotherapy\",\n",
    "        \"hiv\", \"cancer\", \"tumor\",\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for w in data.get(\"results\", []):\n",
    "        title = (w.get(\"title\") or \"\")\n",
    "        t = title.lower()\n",
    "\n",
    "        # se la query contiene \"rag\", evitiamo il significato biologico (RAG gene)\n",
    "        if \"rag\" in query.lower() and any(bad in t for bad in negative_rag_bio):\n",
    "            continue\n",
    "\n",
    "        if mode == \"ai\":\n",
    "            if not any(k in t for k in keywords_ai):\n",
    "                continue\n",
    "\n",
    "        results.append({\n",
    "            \"id\": w.get(\"id\"),\n",
    "            \"doi\": (w.get(\"doi\") or \"\").replace(\"https://doi.org/\", \"\") if w.get(\"doi\") else None,\n",
    "            \"title\": title,\n",
    "            \"publication_year\": w.get(\"publication_year\"),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "            \"primary_location\": (\n",
    "                (w.get(\"primary_location\") or {})\n",
    "                .get(\"source\", {})\n",
    "                .get(\"display_name\")\n",
    "            ),\n",
    "            \"openalex_url\": w.get(\"id\"),\n",
    "        })\n",
    "\n",
    "        if len(results) >= per_page:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "'''\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "(SERVERS := ROOT / \"servers\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(SERVERS / \"arxiv_server.py\").write_text(arxiv_server, encoding=\"utf-8\")\n",
    "(SERVERS / \"openalex_server.py\").write_text(openalex_server, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Creati:\", SERVERS / \"arxiv_server.py\", \"e\", SERVERS / \"openalex_server.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83333d39",
   "metadata": {},
   "source": [
    "## 0.3 – Scrivi un client CLI di test (chiama entrambi i server)\n",
    "\n",
    "Questo client:\n",
    "1) lista i tool disponibili sui due server\n",
    "2) chiama `arxiv_search`\n",
    "3) chiama `openalex_search_works`\n",
    "4) stampa risultati separati + una mini-sintesi (senza LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcd99739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato: /Users/monica.costantini/Documents/workspace/mcp-demo/cli/mcp_chat.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "client_cli = r'''import asyncio\n",
    "import argparse\n",
    "import json\n",
    "import httpx\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StdioTransport\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "async def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"question\", help=\"Es: 'agentic RAG evaluation'\")\n",
    "    ap.add_argument(\"--k\", type=int, default=3, help=\"Risultati per sorgente\")\n",
    "    ap.add_argument(\"--llm\", action=\"store_true\", help=\"Genera risposta finale con LLM locale (Ollama)\")\n",
    "    ap.add_argument(\"--model\", type=str, default=\"llama3.2\", help=\"Modello Ollama (es: llama3.2)\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    arxiv = Client(StdioTransport(command=\"python\", args=[\"servers/arxiv_server.py\"]))\n",
    "    openalex = Client(StdioTransport(command=\"python\", args=[\"servers/openalex_server.py\"]))\n",
    "\n",
    "    async with arxiv, openalex:\n",
    "        tools_a = await arxiv.list_tools()\n",
    "        tools_o = await openalex.list_tools()\n",
    "\n",
    "        print(\"\\n== Tools arXiv ==\")\n",
    "        for t in tools_a:\n",
    "            print(\"-\", t.name)\n",
    "\n",
    "        print(\"\\n== Tools OpenAlex ==\")\n",
    "        for t in tools_o:\n",
    "            print(\"-\", t.name)\n",
    "\n",
    "        ax = await arxiv.call_tool(\"arxiv_search\", {\"topic\": args.question, \"max_results\": args.k})\n",
    "        ox = await openalex.call_tool(\"openalex_search_works\", {\"query\": args.question, \"per_page\": args.k})\n",
    "\n",
    "        def unpack(resp):\n",
    "            content = None\n",
    "            for key in (\"data\", \"result\", \"content\", \"value\"):\n",
    "                if hasattr(resp, key):\n",
    "                    v = getattr(resp, key)\n",
    "                    if v is not None:\n",
    "                        content = v\n",
    "                        break\n",
    "            if content is None:\n",
    "                content = resp\n",
    "\n",
    "            # lista di TextContent\n",
    "            if isinstance(content, list) and content and hasattr(content[0], \"text\"):\n",
    "                text = \"\\n\".join(c.text for c in content if hasattr(c, \"text\"))\n",
    "                try:\n",
    "                    return json.loads(text)\n",
    "                except Exception:\n",
    "                    return text\n",
    "\n",
    "            # singolo TextContent\n",
    "            if hasattr(content, \"text\"):\n",
    "                try:\n",
    "                    return json.loads(content.text)\n",
    "                except Exception:\n",
    "                    return content.text\n",
    "\n",
    "            if isinstance(content, (list, dict)):\n",
    "                return content\n",
    "\n",
    "            return content\n",
    "\n",
    "        ax_items = unpack(ax)\n",
    "        ox_items = unpack(ox)\n",
    "\n",
    "        print(\"\\n==============================\")\n",
    "        print(\"RISULTATI arXiv (preprint)\")\n",
    "        print(\"==============================\")\n",
    "        if isinstance(ax_items, str):\n",
    "            print(ax_items)\n",
    "        elif not ax_items:\n",
    "            print(\"(nessun risultato)\")\n",
    "        else:\n",
    "            for i, p in enumerate(ax_items, 1):\n",
    "                print(f\"{i}. {p.get('title')}\")\n",
    "                print(f\"   PDF: {p.get('pdf_url')}\\n\")\n",
    "\n",
    "        print(\"\\n==============================\")\n",
    "        print(\"RISULTATI OpenAlex (panoramica + citazioni)\")\n",
    "        print(\"==============================\")\n",
    "        if isinstance(ox_items, str):\n",
    "            print(ox_items)\n",
    "        elif not ox_items:\n",
    "            print(\"(nessun risultato)\")\n",
    "        else:\n",
    "            for i, w in enumerate(ox_items, 1):\n",
    "                print(f\"{i}. {w.get('title')}\")\n",
    "                print(f\"   Year: {w.get('publication_year')} | Cited by: {w.get('cited_by_count')}\")\n",
    "                print(f\"   DOI: {w.get('doi')} | Source: {w.get('primary_location')}\")\n",
    "                print(f\"   OpenAlex: {w.get('openalex_url')}\\n\")\n",
    "\n",
    "        if not args.llm:\n",
    "            print(\"\\n=== Sintesi (senza LLM) ===\")\n",
    "            print(\"- arXiv: preprint molto recenti, abstract + PDF.\")\n",
    "            print(\"- OpenAlex: copertura più ampia + segnali come citazioni/anno.\")\n",
    "            print(\"Step successivo: usare un LLM (locale) nel client per fondere i risultati in una risposta unica.\")\n",
    "            return\n",
    "\n",
    "        # --- LLM finale (Ollama) ---\n",
    "        prompt = f\"\"\"\n",
    "Sei un assistente di ricerca scientifica.\n",
    "\n",
    "REGOLE OBBLIGATORIE:\n",
    "1. Usa ESCLUSIVAMENTE le informazioni presenti nelle FONTI fornite.\n",
    "2. OGNI affermazione fattuale nel testo DEVE avere una citazione inline\n",
    "   nel formato [arXiv-1], [arXiv-2], [OpenAlex-1], ecc.\n",
    "3. Se un'informazione NON è supportata dalle fonti, scrivi esplicitamente:\n",
    "   \"Non emergono evidenze dalle fonti fornite.\"\n",
    "4. Non usare conoscenza generale o memoria esterna.\n",
    "\n",
    "DOMANDA:\n",
    "{args.question}\n",
    "\n",
    "RISULTATI arXiv (lista, in ordine):\n",
    "{json.dumps(ax_items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "RISULTATI OpenAlex (lista, in ordine):\n",
    "{json.dumps(ox_items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "OUTPUT RICHIESTO:\n",
    "- Un paragrafo di sintesi con CITAZIONI INLINE obbligatorie\n",
    "- Una sezione finale \"Fonti\" che elenca:\n",
    "  * [arXiv-i] titolo + link PDF\n",
    "  * [OpenAlex-j] titolo + DOI o link OpenAlex\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                OLLAMA_URL,\n",
    "                json={\"model\": args.model, \"prompt\": prompt, \"stream\": False},\n",
    "                timeout=120.0\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            answer = data.get(\"response\", \"\").strip()\n",
    "        except Exception as e:\n",
    "            print(\"\\n[ERRORE LLM] Non riesco a chiamare Ollama.\")\n",
    "            print(\"Assicurati che 'ollama serve' sia attivo e che il modello sia scaricato (es: ollama pull llama3.2).\")\n",
    "            print(\"Dettagli:\", e)\n",
    "            return\n",
    "\n",
    "        print(\"\\n==============================\")\n",
    "        print(\"RISPOSTA (LLM)\")\n",
    "        print(\"==============================\")\n",
    "        print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "(CLI := ROOT / \"cli\").mkdir(parents=True, exist_ok=True)\n",
    "(CLI / \"mcp_chat.py\").write_text(client_cli, encoding=\"utf-8\")\n",
    "print(\"Creato:\", CLI / \"mcp_chat.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a129332",
   "metadata": {},
   "source": [
    "## 0.4 – Configurazione VS Code (MCP servers)\n",
    "\n",
    "Crea `.vscode/mcp.json` così VS Code può avviare i server MCP locali (stdio).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "faa5d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato: /Users/monica.costantini/Documents/workspace/mcp-demo/.vscode/mcp.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "VSCODE = ROOT/\".vscode\"\n",
    "VSCODE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mcp_json = {\n",
    "  \"servers\": {\n",
    "    \"arxiv-local\": {\"command\": \"python\", \"args\": [\"servers/arxiv_server.py\"]},\n",
    "    \"openalex-local\": {\"command\": \"python\", \"args\": [\"servers/openalex_server.py\"]}\n",
    "  }\n",
    "}\n",
    "\n",
    "(VSCODE/\"mcp.json\").write_text(json.dumps(mcp_json, indent=2), encoding=\"utf-8\")\n",
    "print(\"Creato:\", VSCODE/\"mcp.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77ec81",
   "metadata": {},
   "source": [
    "## 0.5 – Dipendenze\n",
    "\n",
    "Nel terminale di VS Code (consigliato), dentro `mcp-demo/`:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "# mac/linux\n",
    "source .venv/bin/activate\n",
    "# windows\n",
    "# .venv\\Scripts\\activate\n",
    "\n",
    "pip install -U pip\n",
    "pip install \"mcp[cli]\" fastmcp arxiv httpx\n",
    "```\n",
    "\n",
    "Poi passa al Notebook 1 per testare il client CLI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
