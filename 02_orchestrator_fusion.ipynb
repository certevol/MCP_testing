{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c61592",
   "metadata": {},
   "source": [
    "# MCP demo – Orchestratore multi-sorgente (arXiv + OpenAlex) + fusione + LLM (opzionale)\n",
    "\n",
    "Questo notebook è l’orchestratore “vero”:\n",
    "- decide quali sorgenti chiamare (routing)\n",
    "- chiama i server MCP (arXiv, OpenAlex)\n",
    "- fonde i risultati\n",
    "- opzionalmente usa un LLM locale (Ollama) per produrre una risposta con citazioni inline verificabili\n",
    "\n",
    "Nota: in Jupyter si usa `await` (no `asyncio.run()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fb1b1",
   "metadata": {},
   "source": [
    "## 2. Cell 2 — Import e utilità (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b080bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import httpx\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StdioTransport\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ed3fd",
   "metadata": {},
   "source": [
    "## 3. Cell 3 — Unpack robusto delle risposte MCP (Python)\n",
    "\n",
    "Questo evita problemi tipo TextContent e formati diversi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3f925f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(resp):\n",
    "    \"\"\"\n",
    "    Estrae payload utile da CallToolResult / TextContent.\n",
    "    Caso chiave: content = [TextContent(json_obj_1), TextContent(json_obj_2), ...]\n",
    "    -> ritorna [dict, dict, ...]\n",
    "    \"\"\"\n",
    "    # 1) prendi il contenuto \"vero\"\n",
    "    content = None\n",
    "    for key in (\"data\", \"result\", \"content\", \"value\", \"structuredContent\"):\n",
    "        if hasattr(resp, key):\n",
    "            v = getattr(resp, key)\n",
    "            if v is not None:\n",
    "                content = v\n",
    "                break\n",
    "    if content is None:\n",
    "        content = resp\n",
    "\n",
    "    # 2) lista di TextContent: parse elemento-per-elemento\n",
    "    if isinstance(content, list) and content and hasattr(content[0], \"text\"):\n",
    "        items = []\n",
    "        for c in content:\n",
    "            if not hasattr(c, \"text\"):\n",
    "                continue\n",
    "            t = (c.text or \"\").strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            try:\n",
    "                items.append(json.loads(t))   # <-- qui risolviamo il tuo caso\n",
    "            except Exception:\n",
    "                items.append(t)\n",
    "        return items\n",
    "\n",
    "    # 3) singolo TextContent\n",
    "    if hasattr(content, \"text\"):\n",
    "        t = (content.text or \"\").strip()\n",
    "        try:\n",
    "            return json.loads(t)\n",
    "        except Exception:\n",
    "            return t\n",
    "\n",
    "    # 4) già list/dict\n",
    "    if isinstance(content, (list, dict)):\n",
    "        return content\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046f1d5",
   "metadata": {},
   "source": [
    "## 4. Cell 4 — Routing: domain detection (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47bf15e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_domain(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Ritorna:\n",
    "      - \"ai\" per domande su RAG/LLM/agentic\n",
    "      - \"general\" per tutto il resto (es: product management)\n",
    "    \"\"\"\n",
    "    q = question.lower()\n",
    "    ai_markers = (\n",
    "        \"rag\", \"retrieval\", \"llm\", \"agent\",\n",
    "        \"transformer\", \"embedding\", \"rerank\",\n",
    "        \"vector\", \"prompt\", \"hallucination\",\n",
    "        \"evaluation\", \"benchmark\"\n",
    "    )\n",
    "    return \"ai\" if any(k in q for k in ai_markers) else \"general\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9756447",
   "metadata": {},
   "source": [
    "## 5. Cell 5 — Policy sorgenti (Python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e43a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_sources(question: str, domain: str) -> Tuple[bool, bool]:\n",
    "    \"\"\"\n",
    "    Decide quali sorgenti chiamare:\n",
    "      - ai     -> arXiv + OpenAlex (mode ai)\n",
    "      - general-> OpenAlex (mode general), arXiv spesso inutile\n",
    "    \"\"\"\n",
    "    if domain == \"general\":\n",
    "        return False, True\n",
    "    return True, True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d099751",
   "metadata": {},
   "source": [
    "## 6. Cell 6 — Client MCP helper (Python) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dc6ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arxiv_client() -> Client:\n",
    "    return Client(StdioTransport(command=\"python\", args=[\"servers/arxiv_server.py\"]))\n",
    "\n",
    "def make_openalex_client() -> Client:\n",
    "    return Client(StdioTransport(command=\"python\", args=[\"servers/openalex_server.py\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffea7bb",
   "metadata": {},
   "source": [
    "## 7. Cell 7 — Fetch robusto con retry + fallback (Python)\n",
    "\n",
    "Qui gestiamo il 429: se arXiv fallisce, continuiamo con OpenAlex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a64c75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def asyncio_sleep(s: float):\n",
    "    await asyncio.sleep(s)\n",
    "\n",
    "async def call_arxiv_safe(arxiv: Client, question: str, k: int) -> List[Dict]:\n",
    "    backoffs = [1.5, 3.0, 6.0]\n",
    "    for attempt, wait_s in enumerate([0.0] + backoffs):\n",
    "        if wait_s:\n",
    "            await asyncio_sleep(wait_s)\n",
    "        try:\n",
    "            ax = await arxiv.call_tool(\"arxiv_search\", {\"topic\": question, \"max_results\": k})\n",
    "            data = unpack(ax)\n",
    "            return data if isinstance(data, list) else []\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if (\"429\" in msg) or (\"rate\" in msg.lower()):\n",
    "                if attempt < len(backoffs):\n",
    "                    continue\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "async def call_openalex(openalex: Client, question: str, k: int, mode: str) -> List[Dict]:\n",
    "    ox = await openalex.call_tool(\n",
    "        \"openalex_search_works\",\n",
    "        {\"query\": question, \"per_page\": k, \"mode\": mode}\n",
    "    )\n",
    "    data = unpack(ox)\n",
    "    return data if isinstance(data, list) else []\n",
    "\n",
    "async def fetch(\n",
    "    question: str,\n",
    "    k: int = 3,\n",
    "    use_arxiv: Optional[bool] = None,\n",
    "    use_openalex: Optional[bool] = None\n",
    ") -> Dict[str, Any]:\n",
    "    domain = detect_domain(question)\n",
    "\n",
    "    # default (se l'utente non decide)\n",
    "    default_arxiv, default_openalex = pick_sources(question, domain)\n",
    "    if use_arxiv is None:\n",
    "        use_arxiv = default_arxiv\n",
    "    if use_openalex is None:\n",
    "        use_openalex = default_openalex\n",
    "\n",
    "    arxiv = make_arxiv_client()\n",
    "    openalex = make_openalex_client()\n",
    "\n",
    "    ax_data: List[Dict] = []\n",
    "    ox_data: List[Dict] = []\n",
    "\n",
    "    ox_mode = \"ai\" if domain == \"ai\" else \"general\"\n",
    "\n",
    "    # Se non attivi nulla\n",
    "    if not use_arxiv and not use_openalex:\n",
    "        return {\"domain\": domain, \"arxiv\": [], \"openalex\": []}\n",
    "\n",
    "    # Solo arXiv\n",
    "    if use_arxiv and not use_openalex:\n",
    "        async with arxiv:\n",
    "            ax_data = await call_arxiv_safe(arxiv, question, k)\n",
    "        return {\"domain\": domain, \"arxiv\": ax_data, \"openalex\": []}\n",
    "\n",
    "    # Solo OpenAlex\n",
    "    if use_openalex and not use_arxiv:\n",
    "        async with openalex:\n",
    "            ox_data = await call_openalex(openalex, question, k, mode=ox_mode)\n",
    "        return {\"domain\": domain, \"arxiv\": [], \"openalex\": ox_data}\n",
    "\n",
    "    # Entrambi\n",
    "    async with arxiv, openalex:\n",
    "        ax_data = await call_arxiv_safe(arxiv, question, k)\n",
    "        ox_data = await call_openalex(openalex, question, k, mode=ox_mode)\n",
    "\n",
    "    return {\"domain\": domain, \"arxiv\": ax_data, \"openalex\": ox_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c2985",
   "metadata": {},
   "source": [
    "## 8. Cell 8 — Fusione “umana” (senza LLM) con citazioni inline (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "122756e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sources_block(ax: List[Dict], ox: List[Dict]) -> str:\n",
    "    lines = []\n",
    "    if ax:\n",
    "        lines.append(\"## Fonti arXiv\")\n",
    "        for i, p in enumerate(ax, 1):\n",
    "            lines.append(f\"- [arXiv-{i}] {p.get('title')}\")\n",
    "            lines.append(f\"  PDF: {p.get('pdf_url')}\")\n",
    "        lines.append(\"\")\n",
    "    if ox:\n",
    "        lines.append(\"## Fonti OpenAlex\")\n",
    "        for i, w in enumerate(ox, 1):\n",
    "            doi = w.get(\"doi\")\n",
    "            doi_link = f\"https://doi.org/{doi}\" if doi else None\n",
    "            lines.append(f\"- [OpenAlex-{i}] {w.get('title')}\")\n",
    "            lines.append(f\"  Year: {w.get('publication_year')} | Cited by: {w.get('cited_by_count')}\")\n",
    "            if doi_link:\n",
    "                lines.append(f\"  DOI: {doi_link}\")\n",
    "            else:\n",
    "                lines.append(f\"  OpenAlex: {w.get('openalex_url')}\")\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def fuse_answer_no_llm(question: str, domain: str, ax: List[Dict], ox: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Risposta 'chatbot style' senza LLM, ma con citazioni inline.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    lines.append(f\"Domanda: {question}\")\n",
    "    lines.append(f\"DOMAIN: {domain}\\n\")\n",
    "\n",
    "    if domain == \"ai\":\n",
    "        if ax:\n",
    "            lines.append(f\"Ho trovato preprint rilevanti su arXiv (es. [arXiv-1]).\")\n",
    "        if ox:\n",
    "            lines.append(f\"Ho trovato anche risultati in OpenAlex con segnali (citazioni/venue) (es. [OpenAlex-1]).\")\n",
    "        if not ax and not ox:\n",
    "            lines.append(\"Non emergono evidenze dalle fonti fornite.\")\n",
    "    else:\n",
    "        if ox:\n",
    "            lines.append(f\"Per una domanda generalista, uso OpenAlex in modalità 'general' (es. [OpenAlex-1]).\")\n",
    "        else:\n",
    "            lines.append(\"Non emergono evidenze dalle fonti fornite.\")\n",
    "\n",
    "    lines.append(\"\\n\" + format_sources_block(ax, ox))\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ae5bb",
   "metadata": {},
   "source": [
    "## 9. Cell 9 — LLM (Ollama) opzionale con citazioni inline + check (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ef08729",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def allowed_citations(ax: List[Dict], ox: List[Dict]) -> List[str]:\n",
    "    allowed = []\n",
    "    allowed += [f\"[arXiv-{i}]\" for i in range(1, len(ax) + 1)]\n",
    "    allowed += [f\"[OpenAlex-{i}]\" for i in range(1, len(ox) + 1)]\n",
    "    return allowed\n",
    "\n",
    "def build_prompt(question: str, ax: List[Dict], ox: List[Dict]) -> str:\n",
    "    allowed = allowed_citations(ax, ox)\n",
    "    allowed_str = \", \".join(allowed) if allowed else \"(nessuna)\"\n",
    "\n",
    "    return f\"\"\"\n",
    "Sei un assistente di ricerca. Devi rispondere SOLO usando le fonti fornite qui sotto.\n",
    "\n",
    "DOMANDA:\n",
    "{question}\n",
    "\n",
    "FONTI arXiv (ordine -> [arXiv-1], [arXiv-2], ...):\n",
    "{json.dumps(ax, ensure_ascii=False, indent=2)}\n",
    "\n",
    "FONTI OpenAlex (ordine -> [OpenAlex-1], [OpenAlex-2], ...):\n",
    "{json.dumps(ox, ensure_ascii=False, indent=2)}\n",
    "\n",
    "REGOLE OBBLIGATORIE (vincolanti):\n",
    "1) Non usare conoscenza generale. Usa SOLO quello che è nelle fonti.\n",
    "2) Ogni affermazione fattuale deve avere una citazione inline.\n",
    "3) PUOI USARE SOLO QUESTE CITAZIONI: {allowed_str}\n",
    "   - Se usi una citazione diversa, la risposta è invalida.\n",
    "4) Se le fonti non supportano una cosa, scrivi: \"Non emergono evidenze dalle fonti fornite.\"\n",
    "5) Stile: italiano chiaro, niente parole inventate. Se non sai, dillo.\n",
    "\n",
    "OUTPUT:\n",
    "- Un paragrafo di sintesi con citazioni inline.\n",
    "- Poi \"Fonti:\" con elenco puntato, link PDF (arXiv) e DOI/OpenAlex (OpenAlex).\n",
    "\"\"\".strip()\n",
    "\n",
    "async def ollama_generate(prompt: str, model: str = \"llama3.2\") -> str:\n",
    "    async with httpx.AsyncClient(timeout=120.0) as client:\n",
    "        r = await client.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return (data.get(\"response\") or \"\").strip()\n",
    "\n",
    "def has_inline_citations(text: str) -> bool:\n",
    "    return bool(re.search(r\"\\[(arXiv|OpenAlex)-\\d+\\]\", text))\n",
    "\n",
    "def validate_citations(text: str, ax: List[Dict], ox: List[Dict]) -> List[str]:\n",
    "    allowed = set(allowed_citations(ax, ox))\n",
    "    found = re.findall(r\"\\[(arXiv|OpenAlex)-(\\d+)\\]\", text)\n",
    "    bad = []\n",
    "    for src, idx in found:\n",
    "        tag = f\"[{src}-{idx}]\"\n",
    "        if tag not in allowed:\n",
    "            bad.append(tag)\n",
    "    return sorted(set(bad))\n",
    "\n",
    "async def answer_with_llm(question: str, ax: List[Dict], ox: List[Dict], model: str = \"llama3.2\") -> str:\n",
    "    prompt = build_prompt(question, ax, ox)\n",
    "\n",
    "    try:\n",
    "        out = await ollama_generate(prompt, model=model)\n",
    "    except Exception as e:\n",
    "        return f\"[ERRORE LLM] Non riesco a chiamare Ollama. Avvia `ollama serve`.\\nDettagli: {e}\"\n",
    "\n",
    "    # 1) check citazioni presenti\n",
    "    if not has_inline_citations(out):\n",
    "        out += \"\\n\\n[WARNING] La risposta non contiene citazioni inline: potrebbe non essere ancorata alle fonti.\"\n",
    "\n",
    "    # 2) check citazioni valide (non inventate)\n",
    "    bad = validate_citations(out, ax, ox)\n",
    "    if bad:\n",
    "        out += f\"\\n\\n[WARNING] Citazioni NON valide (non presenti tra le fonti passate): {bad}\"\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fc895",
   "metadata": {},
   "source": [
    "## 10- Cell 10 — Funzione “answer” orchestratore (Python)\n",
    "\n",
    "Questa è la funzione “unica” che userai sempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d11898b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer(\n",
    "    question: str,\n",
    "    k: int = 3,\n",
    "    use_llm: bool = False,\n",
    "    model: str = \"llama3.2\",\n",
    "    use_arxiv: Optional[bool] = None,\n",
    "    use_openalex: Optional[bool] = None\n",
    ") -> str:\n",
    "    bundle = await fetch(\n",
    "        question,\n",
    "        k=k,\n",
    "        use_arxiv=use_arxiv,\n",
    "        use_openalex=use_openalex\n",
    "    )\n",
    "\n",
    "    domain = bundle[\"domain\"]\n",
    "    ax = bundle[\"arxiv\"]\n",
    "    ox = bundle[\"openalex\"]\n",
    "\n",
    "    if not use_llm:\n",
    "        return fuse_answer_no_llm(question, domain, ax, ox)\n",
    "\n",
    "    return await answer_with_llm(question, ax, ox, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c08d64",
   "metadata": {},
   "source": [
    "## 11. Cell 11 — Esempi di test (Python)\n",
    "\n",
    "⚠️ Questa cella va eseguita con await (Jupyter ok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "186564de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domanda: agentic RAG evaluation\n",
      "DOMAIN: ai\n",
      "\n",
      "Ho trovato preprint rilevanti su arXiv (es. [arXiv-1]).\n",
      "Ho trovato anche risultati in OpenAlex con segnali (citazioni/venue) (es. [OpenAlex-1]).\n",
      "\n",
      "## Fonti arXiv\n",
      "- [arXiv-1] Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation\n",
      "  PDF: https://arxiv.org/pdf/2510.25518v1\n",
      "- [arXiv-2] RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation\n",
      "  PDF: https://arxiv.org/pdf/2502.13957v2\n",
      "- [arXiv-3] MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n",
      "  PDF: https://arxiv.org/pdf/2401.15391v1\n",
      "\n",
      "## Fonti OpenAlex\n",
      "- [OpenAlex-1] Development and evaluation of an agentic LLM based RAG framework for evidence-based patient education\n",
      "  Year: 2025 | Cited by: 1\n",
      "  DOI: https://doi.org/10.1136/bmjhci-2025-101570\n",
      "- [OpenAlex-2] AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\n",
      "  Year: 2025 | Cited by: 35\n",
      "  DOI: https://doi.org/10.70777/si.v2i3.15161\n",
      "- [OpenAlex-3] FinBen: A Holistic Financial Benchmark for Large Language Models\n",
      "  Year: 2024 | Cited by: 21\n",
      "  DOI: https://doi.org/10.48550/arxiv.2402.12659\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Domanda: product management\n",
      "DOMAIN: general\n",
      "\n",
      "Per una domanda generalista, uso OpenAlex in modalità 'general' (es. [OpenAlex-1]).\n",
      "\n",
      "## Fonti OpenAlex\n",
      "- [OpenAlex-1] New Products Management\n",
      "  Year: 1987 | Cited by: 740\n",
      "  OpenAlex: https://openalex.org/W1607031738\n",
      "- [OpenAlex-2] New products management\n",
      "  Year: 1993 | Cited by: 771\n",
      "  DOI: https://doi.org/10.1016/0923-4748(93)90075-t\n",
      "- [OpenAlex-3] Modularity, flexibility, and knowledge management in product and organization design\n",
      "  Year: 1996 | Cited by: 2447\n",
      "  DOI: https://doi.org/10.1002/smj.4250171107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process group termination failed for PID 47081: [Errno 3] No such process, falling back to simple terminate\n"
     ]
    }
   ],
   "source": [
    "out1 = await answer(\"agentic RAG evaluation\", k=3, use_llm=False)\n",
    "print(out1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "out2 = await answer(\"product management\", k=3, use_llm=False)\n",
    "print(out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48531a3",
   "metadata": {},
   "source": [
    "## 12. Cell 12 — Test LLM (Python, opzionale)\n",
    "\n",
    "Richiede Ollama attivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76532568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecco il mio risposta:\n",
      "\n",
      "Rag extentions sono sistemi che migliorano le performance dei modeli di generazione linguaggio utilizzando la ricerca e l'apprendimento automatico. Secondo un recente articolo pubblicato su arXiv ([arXiv-1]), i sistemi RAG-Gym e RAG-Star presentano risultati promettenti nella risoluzione di compiti di ragionamento e generazione linguaggio.\n",
      "\n",
      "In particolare, l'articolo [arXiv-1] introduce il sistema RAG-Gym, che offre una piattaforma completa per la ricerca e l'ottimizzazione dei sistemi RAG. Il sistema propone tre dimensioni di ottimizzazione: prompt engineering, attuator tuning e critic training.\n",
      "\n",
      "Un altro articolo pubblicato su arXiv ([arXiv-2]) si concentra sulla risoluzione di multi-hop queries utilizzando sistemi RAG. L'autore sostiene che gli sistemas esistenti sono insufficienti per risolvere queste complessità.\n",
      "\n",
      "Inoltre, l'articolo [arXiv-3] presenta un sistema chiamato RAG-Star, che integra la ricerca e l'apprendimento automatico per migliorare le performance dei modeli di generazione linguaggio. Il sistema propone una combinazione di ricerca e deliberative reasoning.\n",
      "\n",
      "Fonti:\n",
      "* [arXiv-1]: Guangzhi Xiong et al., \"RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation\", arXiv.org/abs/2502.13957v2, 2025.\n",
      "* [arXiv-2]: Yixuan Tang et al., \"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\", arXiv.org/abs/2401.15391v1, 2024.\n",
      "* [arXiv-3]: Jinhao Jiang et al., \"RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement\", arXiv.org/abs/2412.12881v1, 2024.\n"
     ]
    }
   ],
   "source": [
    "# Mini \"casella\" testuale: scrivi domanda e premi Invio\n",
    "q = input(\"Domanda: \").strip()\n",
    "\n",
    "src = input(\"Sorgenti (a=arxiv, o=openalex, b=both) [b]: \").strip().lower() or \"b\"\n",
    "\n",
    "use_arxiv = (src in [\"a\", \"b\"])\n",
    "use_openalex = (src in [\"o\", \"b\"])\n",
    "\n",
    "use_llm = True\n",
    "k = 3\n",
    "model = \"llama3.2\"\n",
    "\n",
    "out = await answer(q, k=k, use_llm=use_llm, model=model, use_arxiv=use_arxiv, use_openalex=use_openalex)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab7e29",
   "metadata": {},
   "source": [
    "## STEP DI PROVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb68f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['domain', 'arxiv', 'openalex']), 'ai', 3, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bundle = await fetch(\"agentic RAG evaluation\", k=3)\n",
    "bundle.keys(), bundle[\"domain\"], len(bundle[\"arxiv\"]), len(bundle[\"openalex\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6be59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
